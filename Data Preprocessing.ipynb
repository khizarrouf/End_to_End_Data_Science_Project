{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90b70b9-5d9c-45e0-afbf-566331ea453e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# üìå <span style=\"font-size:18px; color:#007acc;\"><b>Introduction</b></span>\n",
    "\n",
    "This notebook is for **demonstration purposes only**. It focuses on data preprocessing and exploratory data analysis (EDA) for loan delinquency prediction. It starts with loading and cleaning the dataset, including handling missing values, detecting and removing outliers. Following this, we perform EDA, visualizing key numerical and categorical variables to better understand the data distributions and relationships. After feature engineering and normalization, the cleaned data is exported for model training and testing. **Note:** This work was conducted on **MS Azure**, so some settings may need adjustments. The dataset was obtained from **[Freddie Mac](https://www.freddiemac.com/)**. \n",
    "\n",
    "üöÄ <span style=\"font-size:18px; color:#e63946;\"><b>Let's get started!</b></span> üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97825e93-4709-468b-a9e5-b9ba07a8cbec",
   "metadata": {},
   "source": [
    "# <span style=\"font-size:18px; color:#007acc;\"><b> Table of Contents</b></span>\n",
    "1. üìä [Import Libraries](#Import-Libraries) \n",
    "2. üìÇ [Data Loading](#Data-Loading)  \n",
    "3. üßπ [Data Cleaning](#Data-Cleaning) \n",
    "4. üõ†Ô∏è [Feature Engineering](#Feature-Engineering) \n",
    "5. üìä [Exploratory Data Analysis](#Exploratory-Data-Analysis) \n",
    "6. üïµÔ∏è‚Äç‚ôÇÔ∏è [Detect Outliers with Scalable Unsupervised Outlier Detection (SUOD) framework](#Detect-Outliers-with-Scalable-Unsupervised-Outlier-Detection-(SUOD)-framework) \n",
    "7. üìè [Feature Normalization](#Feature-normalization) \n",
    "8. üíæ [Export Data for Model Training and Testing](#Export-Data-for-Model-Training-and-Testing) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dae82b-0294-446c-8624-dcbccf27595f",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b> 1. Import Libraries <a id=\"Import-Libraries\"></a> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e86e6-8df5-4137-be82-ef24eb72272c",
   "metadata": {
    "gather": {
     "logged": 1718924316937
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyod\n",
    "import sklearn\n",
    "import scipy\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import category_encoders as ce\n",
    "from suod.models.base import SUOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models import RandomForestRegressor\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d86384-6981-4808-8171-5f0a2f319171",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b> 2. Data Loading <a id=\"Data-Loading\"></a> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199135df-a7b3-4d33-9bf3-9ca4552ecee6",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1718924317123
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "############## CONNECT TO AZURE DATASTORE ##################\n",
    "###########################################################\n",
    "# Replace 'your_connection_string' with your actual Azure Blob Storage connection string\n",
    "connect_str = 'your_connection_string_here'\n",
    "# Instantiate a BlobServiceClient using a connection string\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "# Replace with your actual container name\n",
    "container_name = 'your_container_name_here'\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "############## IMPORT LOAN ORIGINATION DATA ################\n",
    "###########################################################\n",
    "# Define the year for data\n",
    "year = '2004'\n",
    "orifile = 'orig'\n",
    "# Initialize an empty string to accumulate the content of each text file\n",
    "combined_ori_04 = pd.DataFrame()\n",
    "# Get a client to interact with the specified container\n",
    "blobs_2004 = container_client.list_blobs()\n",
    "for blob in blobs_2004:\n",
    "    if keyword in blob.name:\n",
    "         if orifile in blob.name:\n",
    "            print(blob.name)\n",
    "             blob_client = container_client.get_blob_client(blob)\n",
    "             # Download and save the blob temporarily\n",
    "             with open(\"temp_file.txt\", \"wb\") as download_file:\n",
    "                 download_file.write(blob_client.download_blob().readall())\n",
    "             # Read the file into a DataFrame\n",
    "             df = pd.read_csv(\"temp_file.txt\", sep='|')  # Adjust delimiter as needed\n",
    "             new_column_names_origination = ['Credit Score', 'First Payment Date', 'First Time Homebuyer Flag', 'Maturity Date', 'MSA', 'MI%', 'No of Units', 'Occupancy Status', 'CLTV', 'DTI Ratio', 'Ori UPB',\n",
    "                                'Ori LTV', 'Ori Interest Rate', 'Channel', 'PPM Flag', 'Amortization Type', 'Property State', 'Property Type', 'Postal Code', 'Loan Seq Number', 'Loan Purpose',\n",
    "                                'Ori Loan Term', 'Number of Borrowers', 'Seller Name', 'Servicer Name', 'Super Conforming Flag', 'Pre-relief Refinance Loan Seq Number', 'Program Indicator',\n",
    "                                'Relief Refinance Indicator', 'Property Valuation Method', 'I/O Indicator', 'MI Cancellation Indicator'] \n",
    "             df.columns = new_column_names_origination # Original Column Names from Dataset\n",
    "             Reordered_column_origination = ['Loan Seq Number', 'Credit Score', 'First Payment Date', 'First Time Homebuyer Flag', 'Maturity Date', 'MSA', 'MI%', 'No of Units', 'Occupancy Status', 'CLTV', 'DTI Ratio', 'Ori UPB',\n",
    "                                'Ori LTV', 'Ori Interest Rate', 'Channel', 'PPM Flag', 'Amortization Type', 'Property State', 'Property Type', 'Postal Code', 'Loan Purpose',\n",
    "                                'Ori Loan Term', 'Number of Borrowers', 'Seller Name', 'Servicer Name', 'Super Conforming Flag', 'Pre-relief Refinance Loan Seq Number', 'Program Indicator',\n",
    "                                'Relief Refinance Indicator', 'Property Valuation Method', 'I/O Indicator', 'MI Cancellation Indicator']\n",
    "             df = df[Reordered_column_origination]\n",
    "             Reordered_to_keep_ori = ['Loan Seq Number', 'Credit Score', 'First Time Homebuyer Flag', 'MSA', 'No of Units', 'Occupancy Status', 'CLTV', 'DTI Ratio', 'Ori UPB',\n",
    "                                'Ori Interest Rate', 'Channel', 'Amortization Type', 'Property State', 'Property Type', 'Postal Code', 'Loan Purpose',\n",
    "                                'Ori Loan Term', 'Number of Borrowers', 'Servicer Name', 'I/O Indicator', 'MI Cancellation Indicator']\n",
    "             df = df[Reordered_to_keep_ori]\n",
    "             df = df.drop_duplicates(subset='Loan Seq Number', keep='last')\n",
    "             combined_ori_04 = pd.concat([combined_ori_04, df])  # Appending content and a newline to separate files\n",
    "\n",
    "############## IMPORT LOAN PERFORMANCE DATA ###############\n",
    "###########################################################\n",
    "# Define the year for data\n",
    "year = '2004'\n",
    "perffile = 'svcg'\n",
    "# Initialize an empty string to accumulate the content of each text file\n",
    "combined_perf_04 = pd.DataFrame()\n",
    "# Get a client to interact with the specified container\n",
    "blobs_2004 = container_client.list_blobs()\n",
    "for blob in blobs_2004:\n",
    "    if keyword in blob.name:\n",
    "        if perffile in blob.name:\n",
    "            print(blob.name)\n",
    "            blob_client = container_client.get_blob_client(blob)\n",
    "             # Download and save the blob temporarily\n",
    "             with open(\"temp_file.txt\", \"wb\") as download_file:\n",
    "                 download_file.write(blob_client.download_blob().readall())\n",
    "             # Read the file into a DataFrame\n",
    "             df = pd.read_csv(\"temp_file.txt\", sep='|')  # Adjust delimiter as needed\n",
    "             new_column_names_performance = ['Loan Seq Number', 'Monthly reporting period','Current actual UPB','Current loan delinquency status','loan age','Remaining months to legal maturity',\n",
    "                          'Defect settlement data','Modification flag','Zero balance code','ZB effective date','Current interest rate','current deferred UPB','DDLPI','MI recoveries',\n",
    "                          'Net sale proceeds', 'Non MI recoveries', 'Total expenses','Legal costs','Maintenance and preservation costs','Taxes and insurance', 'Miscellaneous expenses',\n",
    "                          'Actual loss calculation','Cumulative modification cost','Step modification flag','Payment deferral','ELTV','ZB removal UPB','Delinquent accrued interest','Delinquency due to disaster',\n",
    "                          'Borrower assistance status code','Current month modification cost','Interest bearing UPB']\n",
    "             df.columns = new_column_names_performance # Original Column Names from Dataset\n",
    "             selected_columns_df_performance = ['Loan Seq Number', 'Current actual UPB', 'Current loan delinquency status', 'loan age', 'Remaining months to legal maturity','Current interest rate']\n",
    "             df = df[selected_columns_df_performance]\n",
    "             # Remove duplicates, keep the last occurrence\n",
    "             df = df.drop_duplicates(subset='Loan Seq Number', keep='last')\n",
    "             # Append to the combined DataFrame\n",
    "             combined_perf_04 = pd.concat([combined_perf_04, df])\n",
    "\n",
    "## MERGE ORIGINATION AND PERFORMANCE DATA ON 'LOAN SEQUENCE NUMBER' ##\n",
    "# Merge data w.r.t this column\n",
    "on_column = 'Loan Seq Number'\n",
    "# Selected columns to keep from both DataFrames\n",
    "selected_columns_df_performance = ['Loan Seq Number', 'Current actual UPB', 'Current loan delinquency status', 'loan age', 'Remaining months to legal maturity','Current interest rate']\n",
    "selected_columns_df_performance = combined_perf_04[selected_columns_df_performance]\n",
    "# Merge DataFrames on the specified column\n",
    "merged_df = pd.merge(combined_ori_04, selected_columns_df_performance, on=on_column, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67030e4-a765-4bcf-a83d-20d761c095a4",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b> 3. Data Cleaning <a id=\"Data-Cleaning\"></a> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1d37b8-291c-43fb-a57f-3d4af444281e",
   "metadata": {
    "gather": {
     "logged": 1718924318967
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################### CLEAN ORIGINATION DATA ##############\n",
    "###########################################################\n",
    "# Fill missing values for credit score with mean\n",
    "valid_scores = merged_df[(merged_df['Credit Score'] >= 300) & (merged_df['Credit Score'] <= 900)]['Credit Score']\n",
    "mean_score = valid_scores.mean()\n",
    "merged_df['Credit Score'].fillna(mean_score, inplace=True)\n",
    "\n",
    "# Fill missing values for FTHB flag with mode\n",
    "mode_value = merged_df['First Time Homebuyer Flag'].mode()[0] \n",
    "merged_df['First Time Homebuyer Flag'].fillna(mode_value, inplace=True)\n",
    "\n",
    "# Fill missing values with mode\n",
    "merged_df['MSA'].fillna(0, inplace=True)\n",
    "merged_df.loc[merged_df['MSA'] != 0, 'MSA'] = 1\n",
    "\n",
    "# Fill missing values for number of units\n",
    "mode_units = merged_df['No of Units'].mode()[0]\n",
    "merged_df.loc[merged_df['No of Units'].isnull() | (merged_df['No of Units'] > 4), 'No of Units'] = mode_units\n",
    "\n",
    "#Fill missing values for Occupancy status\n",
    "#merged_df['Occupancy Status'].replace(9, np.nan, inplace=True)\n",
    "mode_value = merged_df['Occupancy Status'].mode()[0]\n",
    "merged_df['Occupancy Status'].fillna(mode_value, inplace=True)\n",
    "#print(merged_df['Occupancy Status'])\n",
    "\n",
    "#Fill missing values for CLTV\n",
    "mean_score = merged_df['CLTV'].mean()\n",
    "merged_df['CLTV'].fillna(mean_score, inplace=True)\n",
    "\n",
    "#Fill missing values for DTI\n",
    "valid_DTI = merged_df[(merged_df['DTI Ratio'] >= 0) & (merged_df['DTI Ratio'] <= 100)]['DTI Ratio']\n",
    "mean_score = valid_DTI.mean()\n",
    "thershold = 100\n",
    "merged_df.loc[merged_df['DTI Ratio'] > thershold, 'DTI Ratio'] = np.nan\n",
    "merged_df['DTI Ratio'].fillna(mean_score, inplace=True)\n",
    "\n",
    "#Fill missing values for O_UPB\n",
    "mean_score = merged_df['Ori UPB'].mean()\n",
    "merged_df['Ori UPB'].fillna(mean_score, inplace=True)\n",
    "\n",
    "#Fill missing values for Original Interest Rate\n",
    "# For variable, check the bank rate, for fixed, take mean \n",
    "mean_score = merged_df['Ori Interest Rate'].mean()\n",
    "merged_df['Ori Interest Rate'].fillna(mean_score, inplace=True)\n",
    "\n",
    "#Fill missing values for Channel \n",
    "#merged_df['Channel'].replace(9, np.nan, inplace=True)\n",
    "mode_value = merged_df['Channel'].mode()[0] \n",
    "merged_df['Channel'].fillna(mode_value, inplace=True)\n",
    "\n",
    "#Fill missing values for Amortization type\n",
    "mode_value = merged_df['Amortization Type'].mode()[0] \n",
    "merged_df['Amortization Type'].fillna(mode_value, inplace=True)\n",
    "\n",
    "#Fill missing values for Property state\n",
    "mode_value = merged_df['Property State'].mode()[0] \n",
    "merged_df['Property State'].fillna(mode_value, inplace=True)\n",
    "\n",
    "#Fill missing values for Property type\n",
    "merged_df['Property Type'].replace(99, np.nan, inplace=True)\n",
    "mode_value = merged_df['Property Type'].mode()[0] \n",
    "merged_df['Property Type'].fillna(mode_value, inplace=True)\n",
    "\n",
    "#Fill missing values for Loan Purpose\n",
    "merged_df['Loan Purpose'].replace(9, np.nan, inplace=True)\n",
    "mode_value = merged_df['Loan Purpose'].mode()[0] \n",
    "merged_df['Loan Purpose'].fillna(mode_value, inplace=True)\n",
    "\n",
    "#Fill missing values for Original Loan Term\n",
    "mode_value = merged_df['Ori Loan Term'].mode()[0] \n",
    "merged_df['Ori Loan Term'].fillna(mode_value, inplace=True)\n",
    "\n",
    "#Fill missing values for Number of Borrowers\n",
    "merged_df['Number of Borrowers'].replace(99, np.nan, inplace=True)\n",
    "mode_value = merged_df['Number of Borrowers'].mode()[0] \n",
    "merged_df['Number of Borrowers'].fillna(mode_value, inplace=True)\n",
    "\n",
    "#Fill missing values for Servicer Name\n",
    "mode_value = merged_df['Servicer Name'].mode()[0] \n",
    "merged_df['Servicer Name'].fillna(mode_value, inplace=True)\n",
    "\n",
    "#Fill missing values for I/O Indicator\n",
    "mode_value = merged_df['I/O Indicator'].mode()[0] \n",
    "merged_df['I/O Indicator'].fillna(mode_value, inplace=True)\n",
    "\n",
    "#Fill missing values for MI Cancellation Indicator\n",
    "merged_df['MI Cancellation Indicator'].replace('7', np.nan, inplace=True)\n",
    "mode_value = merged_df['MI Cancellation Indicator'].mode()[0] \n",
    "merged_df['MI Cancellation Indicator'].fillna(mode_value, inplace=True)\n",
    "#print(merged_df['MI Cancellation Indicator'])\n",
    "\n",
    "############## CLEAN PERFORMANCE DATA #################\n",
    "#######################################################\n",
    "\n",
    "#Fill missing values for Current Actual UPB\n",
    "mode_value = merged_df['Current actual UPB'].mode()[0] \n",
    "merged_df['Current actual UPB'].fillna(mode_value, inplace=True)\n",
    "\n",
    "#Fill missing values for Current Loan Deliquency Status\n",
    "value_to_assign = 4 \n",
    "for index, value in merged_df['Current loan delinquency status'].items():\n",
    "    if str(value).isalpha():\n",
    "        merged_df.at[index, 'Current loan delinquency status'] = value_to_assign\n",
    "mode_value = merged_df['Current loan delinquency status'].mode()[0] \n",
    "merged_df['Current loan delinquency status'].fillna(mode_value, inplace=True)\n",
    "\n",
    "#Fill missing values for Loan Age\n",
    "mean_value = merged_df['loan age'].mean() \n",
    "merged_df['loan age'].fillna(mean_value, inplace=True)\n",
    "\n",
    "#Fill missing values for Remaining Months for Maturity\n",
    "mean_value = merged_df['Remaining months to legal maturity'].mean()\n",
    "merged_df['Remaining months to legal maturity'].fillna(mean_value, inplace=True)\n",
    "\n",
    "#Fill missing values for Current Interest Rate\n",
    "mean_value = merged_df['Current interest rate'].mode()[0] \n",
    "merged_df['Current interest rate'].fillna(mean_value, inplace=True)\n",
    "\n",
    "## CHECKING FOR MISSING VALUES ##\n",
    "print(\"\\nMissing Values:\")\n",
    "print(merged_df.isnull().sum())\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(merged_df.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title('Missing Data Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92cf2b3-ada8-46f2-960d-17b6ae96155b",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b>4. Feature Engineering <a id=\"Feature-Engineering\"></a> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf7be5-d2c8-4181-b828-90222c1c6b75",
   "metadata": {
    "gather": {
     "logged": 1718924319019
    }
   },
   "outputs": [],
   "source": [
    "############## First Time Homebuyer Flag ##################\n",
    "###########################################################\n",
    "# Initialize OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "encoded_data = one_hot_encoder.fit_transform(merged_df[['First Time Homebuyer Flag']])\n",
    "#Convert the result to a DataFrame\n",
    "df_encoded = pd.DataFrame(encoded_data.toarray(), columns=one_hot_encoder.get_feature_names_out(['First Time Homebuyer Flag']))\n",
    "# Drop the original column\n",
    "merged_df.drop(columns=['First Time Homebuyer Flag'], inplace=True)\n",
    "# Concatenate the encoded columns with the original DataFrame\n",
    "merged_df = pd.concat([merged_df, df_encoded], axis=1)\n",
    "if 'First Time Homebuyer Flag_N' in merged_df.columns: #This column is dropped to avoid feature's linear dependency\n",
    "    merged_df.drop(columns=['First Time Homebuyer Flag_N'], inplace=True)\n",
    "new_column_name = {'First Time Homebuyer Flag_Y': 'First Time Homebuyer Flag'}\n",
    "merged_df.rename(columns=new_column_name, inplace=True)\n",
    "\n",
    "############## Occupancy Status ###########################\n",
    "###########################################################\n",
    "# Initialize OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "encoded_data = one_hot_encoder.fit_transform(merged_df[['Occupancy Status']])\n",
    "#Convert the result to a DataFrame\n",
    "df_encoded = pd.DataFrame(encoded_data.toarray(), columns=one_hot_encoder.get_feature_names_out(['Occupancy Status']))\n",
    "# Drop the original column\n",
    "merged_df.drop(columns=['Occupancy Status'], inplace=True)\n",
    "# Concatenate the encoded columns with the original DataFrame\n",
    "merged_df = pd.concat([merged_df, df_encoded], axis=1)\n",
    "merged_df.drop(columns=['Occupancy Status_S'], inplace=True)\n",
    "new_column_name = {'Occupancy Status_I': 'Occupancy Investor'}\n",
    "merged_df.rename(columns=new_column_name, inplace=True)\n",
    "new_column_name = {'Occupancy Status_P': 'Occupancy Principal'}\n",
    "merged_df.rename(columns=new_column_name, inplace=True)\n",
    "\n",
    "############## Amortization Type ##########################\n",
    "###########################################################\n",
    "# Initialize OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "encoded_data = one_hot_encoder.fit_transform(merged_df[['Amortization Type']])\n",
    "#Convert the result to a DataFrame\n",
    "df_encoded = pd.DataFrame(encoded_data.toarray(), columns=one_hot_encoder.get_feature_names_out(['Amortization Type']))\n",
    "# Drop the original column\n",
    "merged_df.drop(columns=['Amortization Type'], inplace=True, axis = ['Columns'])\n",
    "# Concatenate the encoded columns with the original DataFrame\n",
    "merged_df = pd.concat([merged_df, df_encoded], axis=1)\n",
    "if 'Amortization Type_ARM' in merged_df.columns:\n",
    "    merged_df.drop(columns=['Amortization Type_ARM'], inplace=True)\n",
    "new_column_name = {'Amortization Type_FRM': 'Amortization Type'}\n",
    "merged_df.rename(columns=new_column_name, inplace=True)\n",
    "\n",
    "############## Property State ############################\n",
    "###########################################################\n",
    "# Initialize OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "encoded_data = one_hot_encoder.fit_transform(merged_df[['Property State']])\n",
    "#Convert the result to a DataFrame\n",
    "df_encoded = pd.DataFrame(encoded_data.toarray(), columns=one_hot_encoder.get_feature_names_out(['Property State']))\n",
    "# Drop the original column\n",
    "merged_df.drop(columns=['Property State'], inplace=True)\n",
    "# Concatenate the encoded columns with the original DataFrame\n",
    "merged_df = pd.concat([merged_df, df_encoded], axis=1)\n",
    "if 'Property State_AK' in merged_df.columns:\n",
    "    merged_df.drop(columns=['Property State_AK'], inplace=True)\n",
    "\n",
    "############## Property Type ##################\n",
    "###########################################################\n",
    "# Initialize OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "encoded_data = one_hot_encoder.fit_transform(merged_df[['Property Type']])\n",
    "#Convert the result to a DataFrame\n",
    "df_encoded = pd.DataFrame(encoded_data.toarray(), columns=one_hot_encoder.get_feature_names_out(['Property Type']))\n",
    "# Drop the original column\n",
    "merged_df.drop(columns=['Property Type'], inplace=True)\n",
    "# Concatenate the encoded columns with the original DataFrame\n",
    "merged_df = pd.concat([merged_df, df_encoded], axis=1)\n",
    "if 'Property Type_SF' in merged_df.columns:\n",
    "    merged_df.drop(columns=['Property Type_SF'], inplace=True)\n",
    "\n",
    "############## Loan Purpose ##################\n",
    "###########################################################\n",
    "# Initialize OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "encoded_data = one_hot_encoder.fit_transform(merged_df[['Loan Purpose']])\n",
    "#Convert the result to a DataFrame\n",
    "df_encoded = pd.DataFrame(encoded_data.toarray(), columns=one_hot_encoder.get_feature_names_out(['Loan Purpose']))\n",
    "# Drop the original column\n",
    "merged_df.drop(columns=['Loan Purpose'], inplace=True)\n",
    "# Concatenate the encoded columns with the original DataFrame\n",
    "merged_df = pd.concat([merged_df, df_encoded], axis=1)\n",
    "if 'Loan Purpose_R' in merged_df.columns:\n",
    "    merged_df.drop(columns=['Loan Purpose_R'], inplace=True)\n",
    "new_column_name = {'Loan Purpose_P': 'Purchase'}\n",
    "merged_df.rename(columns=new_column_name, inplace=True)\n",
    "new_column_name = {'Loan Purpose_C': 'Cash'}\n",
    "merged_df.rename(columns=new_column_name, inplace=True)\n",
    "new_column_name = {'Loan Purpose_N': 'No Cash Out'}\n",
    "merged_df.rename(columns=new_column_name, inplace=True)\n",
    "\n",
    "############## Servicer Name ##################\n",
    "###########################################################\n",
    "# Initialize OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "encoded_data = one_hot_encoder.fit_transform(merged_df[['Servicer Name']])\n",
    "#Convert the result to a DataFrame\n",
    "df_encoded = pd.DataFrame(encoded_data.toarray(), columns=one_hot_encoder.get_feature_names_out(['Servicer Name']))\n",
    "# Drop the original column\n",
    "merged_df.drop(columns=['Servicer Name'], inplace=True)\n",
    "# Concatenate the encoded columns with the original DataFrame\n",
    "merged_df = pd.concat([merged_df, df_encoded], axis=1)\n",
    "if 'Servicer Name_Other servicers' in merged_df.columns:\n",
    "    merged_df.drop(columns=['Servicer Name_Other servicers'], inplace=True)\n",
    "\n",
    "############## Channel ##################\n",
    "###########################################################\n",
    "# Initialize OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "encoded_data = one_hot_encoder.fit_transform(merged_df[['Channel']])\n",
    "#Convert the result to a DataFrame\n",
    "df_encoded = pd.DataFrame(encoded_data.toarray(), columns=one_hot_encoder.get_feature_names_out(['Channel']))\n",
    "# Drop the original column\n",
    "merged_df.drop(columns=['Channel'], inplace=True)\n",
    "# Concatenate the encoded columns with the original DataFrame\n",
    "merged_df = pd.concat([merged_df, df_encoded], axis=1)\n",
    "if 'Channel_TBO' in merged_df.columns:\n",
    "    merged_df.drop(columns=['Channel_TBO'], inplace=True)\n",
    "\n",
    "############## I/O Indicator ##################\n",
    "###########################################################\n",
    "# Initialize OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "encoded_data = one_hot_encoder.fit_transform(merged_df[['I/O Indicator']])\n",
    "#Convert the result to a DataFrame\n",
    "df_encoded = pd.DataFrame(encoded_data.toarray(), columns=one_hot_encoder.get_feature_names_out(['I/O Indicator']))\n",
    "# Drop the original column\n",
    "merged_df.drop(columns=['I/O Indicator'], inplace=True)\n",
    "# Concatenate the encoded columns with the original DataFrame\n",
    "merged_df = pd.concat([merged_df, df_encoded], axis=1)\n",
    "if 'I/O Indicator_Y' in merged_df.columns:\n",
    "    merged_df.drop(columns=['I/O Indicator_Y'], inplace=True)\n",
    "new_column_name = {'I/O Indicator_N': 'I/O Indicator'}\n",
    "merged_df.rename(columns=new_column_name, inplace=True)\n",
    "\n",
    "############## MI Cancellation Indicator ##################\n",
    "###########################################################\n",
    "# Initialize OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "encoded_data = one_hot_encoder.fit_transform(merged_df[['MI Cancellation Indicator']])\n",
    "#Convert the result to a DataFrame\n",
    "df_encoded = pd.DataFrame(encoded_data.toarray(), columns=one_hot_encoder.get_feature_names_out(['MI Cancellation Indicator']))\n",
    "# Drop the original column\n",
    "merged_df.drop(columns=['MI Cancellation Indicator'], inplace=True)\n",
    "# Concatenate the encoded columns with the original DataFrame\n",
    "merged_df = pd.concat([merged_df, df_encoded], axis=1)\n",
    "if 'MI Cancellation Indicator_N' in merged_df.columns:\n",
    "    merged_df.drop(columns=['MI Cancellation Indicator_N'], inplace=True)\n",
    "new_column_name = {'MI Cancellation Indicator_Y': 'MI Cancellation Indicator_Y'}\n",
    "merged_df.rename(columns=new_column_name, inplace=True)\n",
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2494525e-fc64-444a-9e8b-ddb2467f69fb",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b> 5. Exploratory Data Analysis <a id=\"Exploratory-Data-Analysis\"></a> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2b5cf-8eb8-424c-972c-c5a0f24b148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 1. Check basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(merged_df.info())\n",
    "\n",
    "# 2. Summary statistics of numerical columns\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(merged_df.describe())\n",
    "\n",
    "# 5. Distribution of Credit Score (and other numerical columns)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(merged_df['Credit Score'], kde=True, color='skyblue', bins=50)\n",
    "plt.title('Distribution of Credit Score')\n",
    "plt.xlabel('Credit Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# 6. Distribution of Loan Amount (Ori UPB)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(merged_df['Ori UPB'], kde=True, color='salmon', bins=50)\n",
    "plt.title('Distribution of Original Loan Amount (Ori UPB)')\n",
    "plt.xlabel('Original Loan Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# 7. Count plot for categorical columns (e.g., First Time Homebuyer Flag)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=merged_df, x='First Time Homebuyer Flag', palette='Set2')\n",
    "plt.title('First Time Homebuyer Flag Distribution')\n",
    "plt.xlabel('First Time Homebuyer Flag')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# 8. Correlation heatmap (for numerical columns)\n",
    "plt.figure(figsize=(12, 8))\n",
    "corr = merged_df.corr()  # Make sure to use only numerical columns\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# 9. Pair plot to visualize relationships between numerical variables\n",
    "sns.pairplot(merged_df[['Credit Score', 'Ori UPB', 'DTI Ratio', 'CLTV', 'loan age']])\n",
    "plt.suptitle('Pairplot of Numerical Variables', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# 10. Boxplot for potential outliers in numerical columns\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=merged_df[['Credit Score', 'Ori UPB', 'DTI Ratio', 'CLTV', 'loan age']])\n",
    "plt.title('Boxplot of Numerical Variables')\n",
    "plt.show()\n",
    "\n",
    "# 11. Checking distribution of Loan Delinquency Status\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=merged_df, x='Current loan delinquency status', palette='Set1')\n",
    "plt.title('Loan Delinquency Status Distribution')\n",
    "plt.xlabel('Loan Delinquency Status')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# 12. Visualize relationship between Credit Score and DTI Ratio\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(data=merged_df, x='Credit Score', y='DTI Ratio', alpha=0.6)\n",
    "plt.title('Credit Score vs DTI Ratio')\n",
    "plt.xlabel('Credit Score')\n",
    "plt.ylabel('DTI Ratio')\n",
    "plt.show()\n",
    "\n",
    "# 13. Visualize relationship between Ori UPB and CLTV\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(data=merged_df, x='Ori UPB', y='CLTV', alpha=0.6)\n",
    "plt.title('Original Loan Amount vs CLTV')\n",
    "plt.xlabel('Original Loan Amount')\n",
    "plt.ylabel('CLTV')\n",
    "plt.show()\n",
    "\n",
    "# 14. Checking for duplicates and dropping them if necessary\n",
    "print(\"\\nDuplicate Rows:\")\n",
    "print(merged_df.duplicated().sum())\n",
    "\n",
    "# 15. Explore categorical variables relationship with numerical variables\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=merged_df, x='Loan Purpose', y='Credit Score')\n",
    "plt.title('Credit Score by Loan Purpose')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ea9ae-d078-4037-a1ec-82756d4d85d2",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b> 6. Detect Outliers with Scalable Unsupervised Outlier Detection (SUOD) framework <a id=\"Detect-Outliers-with-Scalable-Unsupervised-Outlier-Detection-(SUOD)-framework\"></a> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766fd7a-c2a1-4570-abb5-380526aa314b",
   "metadata": {
    "gather": {
     "logged": 1718924319123
    }
   },
   "outputs": [],
   "source": [
    "######## DETECT OUTLIERS WITH SUOD ########\n",
    "###########################################\n",
    "# Numerical Data\n",
    "anomaly_inputs = ['Credit Score', 'CLTV', 'No of Units', 'DTI Ratio', 'Ori UPB', 'Ori Interest Rate', 'Ori Loan Term', \n",
    "                  'Number of Borrowers', 'Current actual UPB', 'Current loan delinquency status', 'loan age', \n",
    "                  'Remaining months to legal maturity', 'Current interest rate']\n",
    "\n",
    "X_train = merged_df[anomaly_inputs]\n",
    "print(\"step1\")\n",
    "# Set up your base estimators\n",
    "contamination = 0.01  # 1% of the data are outliers\n",
    "base_estimators = [#IForest(n_estimators=200),\n",
    "    #IForest(n_estimators=100)\n",
    "    #LOF(n_neighbors=5, contamination=contamination),\n",
    "    #LOF(n_neighbors=15, contamination=contamination),\n",
    "    #LOF(n_neighbors=25, contamination=contamination),\n",
    "    #HBOS(contamination=contamination),\n",
    "    #OCSVM(contamination=contamination),\n",
    "    #KNN(n_neighbors=5, contamination=contamination),\n",
    "    #KNN(n_neighbors=15, contamination=contamination),\n",
    "    KNN(n_neighbors=25, contamination=contamination)\n",
    "    ]  \n",
    "# Initialize SUOD model\n",
    "model = SUOD(base_estimators=base_estimators, n_jobs=1,\n",
    "             rp_flag_global=True, bps_flag=True,\n",
    "             approx_flag_global=True, contamination=contamination)\n",
    "# Fit the model\n",
    "model.fit(X_train)\n",
    "model.approximate(X_train)  # conduct model approximation if it is enabled\n",
    "# Predict labels for each base estimator\n",
    "predicted_labels = model.predict(X_train)\n",
    "# Calculate the average of predicted labels across all estimators\n",
    "average_labels = np.mean(predicted_labels, axis=1)\n",
    "# Add the average labels to X_train\n",
    "X_train_with_avg_labels = np.hstack((X_train, average_labels.reshape(-1, 1)))\n",
    "# Separate outliers and inliers based on the average labels\n",
    "outliers_avg = X_train_with_avg_labels[average_labels >= 0.01]\n",
    "inliers_avg = X_train_with_avg_labels[average_labels < 0.01]\n",
    "def convert_to_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        try:\n",
    "            return float(value.replace(\"'\", \"\"))  # Handle string representations of floats\n",
    "        except ValueError:\n",
    "            return np.nan  # Handle non-convertible values, you can modify this as needed\n",
    "inliers_avg = np.vectorize(convert_to_float)(inliers_avg)\n",
    "outliers_avg= np.vectorize(convert_to_float)(outliers_avg)\n",
    "print(outliers_avg.shape)\n",
    "print(inliers_avg.shape)\n",
    "\n",
    "# Funciton to plot outliers\n",
    "def plot_outliers_inliers(outliers, inliers, x_col, y_col, x_label, y_label, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Plot outliers\n",
    "    plt.scatter(outliers[:, x_col], outliers[:, y_col], color='red', label='Outliers (Avg)')\n",
    "    # Plot inliers\n",
    "    plt.scatter(inliers[:, x_col], inliers[:, y_col], color='green', label='Inliers (Avg)')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Define column indices for each plot\n",
    "columns = [(0, 1), (2, 3), (4, 5), (6, 7), (8, 9), (10, 11)]\n",
    "labels = [\n",
    "    ('Credit', 'CLTV'),\n",
    "    ('No of Units', 'DTI_Ratio'),\n",
    "    ('Ori UPB', 'Ori Interest Rate'),\n",
    "    ('Ori Loan Term', 'Number of Borrowers'),\n",
    "    ('Current actual UPB', 'Current loan delinquency status'),\n",
    "    ('Loan age', 'Remaining months to legal maturity')\n",
    "]\n",
    "titles = [\n",
    "    'Outliers and Inliers (Average) - Credit vs CLTV',\n",
    "    'Outliers and Inliers (Average) - No of Units vs DTI_Ratio',\n",
    "    'Outliers and Inliers (Average) - Ori UPB vs Ori Interest Rate',\n",
    "    'Outliers and Inliers (Average) - Ori Loan Term vs Number of Borrowers',\n",
    "    'Outliers and Inliers (Average) - Current actual UPB vs Current loan delinquency status',\n",
    "    'Outliers and Inliers (Average) - Loan age vs Remaining months to legal maturity'\n",
    "]\n",
    "\n",
    "# Loop through each set of column indices and plot\n",
    "for (x_col, y_col), (labels_pair, title) in zip(columns, zip(labels, titles)):\n",
    "    plot_outliers_inliers(outliers_avg, inliers_avg, x_col, y_col, labels_pair[0], labels_pair[1], title)\n",
    "\n",
    "######## ADD OUTLIER COLUMN TO DATASET ########\n",
    "###############################################\n",
    "df_OD = pd.DataFrame(average_labels)\n",
    "merged_df_OD = pd.concat([merged_df, df_OD], axis=1)\n",
    "df_merged_OD = pd.DataFrame(merged_df_OD)\n",
    "\n",
    "# Check for null values\n",
    "null_values = df_merged_OD.isnull().values.any()\n",
    "# Check for NaN values\n",
    "nan_values = df_merged_OD.isna().values.any()\n",
    "if null_values or nan_values:\n",
    "    print(\"There are null or NaN values in the DataFrame.\")\n",
    "else:\n",
    "    print(\"There are no null or NaN values in the DataFrame.\")\n",
    "df_merged_OD.shape\n",
    "\n",
    "######## DELETE ROWS BASED ON OUTLIERS ########\n",
    "###############################################\n",
    "# Remove outliers from the original dataset\n",
    "df_merged_OD_WO_Outliers = df_merged_OD[df_merged_OD.iloc[:, -1] < 0.5]\n",
    "df_merged_OD_WO_Outliers.head()\n",
    "print(df_merged_OD_WO_Outliers.shape)\n",
    "unique_values = df_merged_OD_WO_Outliers['Current loan delinquency status'].unique()\n",
    "print(f\"Unique values in column {'Current loan delinquency status'}: {unique_values}\")\n",
    "\n",
    "######## REPLACE CURRENT LOAN DELIQUENCY STATUS > 1 with 1 ########\n",
    "####################################################################\n",
    "# Clip values greater than 1 in the 'Current loan delinquency status' column\n",
    "df_merged_OD_WO_Outliers['Current loan delinquency status'] = df_merged_OD_WO_Outliers['Current loan delinquency status'].clip(upper=1)\n",
    "\n",
    "# Count occurrences of each value in the 'Current loan delinquency status' column\n",
    "status_counts = df_merged_OD_WO_Outliers['Current loan delinquency status'].value_counts()\n",
    "\n",
    "# Print counts for each status directly\n",
    "status_counts.apply(lambda count, status: print(f\"Number of {status}s in column 'B': {count}\"), \n",
    "                    status=status_counts.index, \n",
    "                    count=status_counts.values)\n",
    "\n",
    "# Get the position of 'Current loan delinquency status' column\n",
    "position = df_merged_OD_WO_Outliers.columns.get_loc('Current loan delinquency status')\n",
    "print(f\"Position of 'Current loan delinquency status' column: {position}\")\n",
    "\n",
    "# Shape of the final dataframe\n",
    "print(f\"Shape of the final dataframe: {df_merged_OD_WO_Outliers.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84bb591-c861-42cf-ba15-ca29d7b27355",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b>7. Feature Normalization <a id=\"Feature-normalization\"></a> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5015b860-6712-4f4c-a1a0-dd6787908807",
   "metadata": {
    "gather": {
     "logged": 1718924319277
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Columns to be normalized\n",
    "columns_to_normalize = ['Credit Score', 'CLTV', 'No of Units', 'DTI Ratio', 'Ori UPB', 'Ori Interest Rate', 'Ori Loan Term', \n",
    "                  'Number of Borrowers', 'Current actual UPB', 'Current loan delinquency status', 'loan age', \n",
    "                  'Remaining months to legal maturity', 'Current interest rate']\n",
    "\n",
    "# Initialize StandardScaler\n",
    "normalizer = MinMaxScaler()\n",
    "\n",
    "# Normalize selected columns\n",
    "df_merged_OD_WO_Outliers[columns_to_normalize] = normalizer.fit_transform(df_merged_OD_WO_Outliers[columns_to_normalize])\n",
    "unique_values = df_merged_OD_WO_Outliers['loan age'].unique()\n",
    "print(f\"Unique values in column {'loan age'}: {unique_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8073660f-a0a4-4044-b311-131639614f7c",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b>8. Export Data for Model Training and Testing <a id=\"Export-Data-for-Model-Training-and-Testing\"></a> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d1bcf9-2aa8-469b-be12-1ce0ea6ea327",
   "metadata": {
    "gather": {
     "logged": 1718924319297
    }
   },
   "outputs": [],
   "source": [
    "##### DROP OUTLIER LABEL COLUMN #####\n",
    "#####################################\n",
    "df_merged_OD_WO_Outliers = df_merged_OD_WO_Outliers.iloc[:, :-1]\n",
    "# Drop the 'Loan Seq Number' column (serial number column)\n",
    "df_merged_OD_WO_Outliers.drop(columns='Loan Seq Number', inplace=True)\n",
    "# Drop the 'Postal Code' column\n",
    "df_merged_OD_WO_Outliers.drop(columns='Postal Code', inplace=True)\n",
    "# Print the shape of the final cleaned dataframe\n",
    "print(f\"Shape of cleaned dataframe: {df_merged_OD_WO_Outliers.shape}\")\n",
    "output_df = df_merged_OD_WO_Outliers[['Current loan delinquency status']]\n",
    "output_df['Current loan delinquency status'] = output_df['Current loan delinquency status'].replace({0: 1, 1: 0})\n",
    "# Drop the 'Current loan delinquency status' column from the main dataframe\n",
    "df_merged_OD_WO_Outliers.drop(columns=['Current loan delinquency status'], inplace=True)\n",
    "\n",
    "###### WRITE DATA TO EXCEL FILES ######\n",
    "######################################\n",
    "input_output_files = [\n",
    "    ('feature_file_0405.xlsx', df_merged_OD_WO_Outliers),  # Features file\n",
    "    ('label_file_0405.xlsx', output_df)  # Labels file\n",
    "]\n",
    "# Write each DataFrame to a separate Excel file\n",
    "for file_name, data_frame in input_output_files:\n",
    "    data_frame.to_excel(file_name, index=False)\n",
    "print(f\"Feature and label files have been saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
