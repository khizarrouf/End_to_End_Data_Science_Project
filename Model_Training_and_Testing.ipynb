{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ <span style=\"font-size:18px; color:#007acc;\"><b>Introduction</b></span>\n",
    "\n",
    "This is the second notebook in the series. The first notebook focused on data preprocessing and cleaning, including handling missing values, detecting and removing outliers, and performing exploratory data analysis (EDA). In this notebook, we continue from that point by importing necessary modules and defining a custom dataset class to handle the loading and preprocessing of the data. We then proceed to define the neural network architecture, followed by dataset instantiation and preprocessing, which includes feature engineering, normalization, and further data preparation. A custom batch sampler is implemented for efficient data loading. The notebook also includes the creation of training and validation functions, defining key evaluation metrics for model performance, and the training and evaluation of the neural network. For interpretability, SHAP analysis is performed to explain feature importance. The trained model is then saved for future use, and a FastAPI server is created to serve the model for local inference. Finally, predictions can be made by sending requests to the API. Note: This work was conducted on MS Azure, so some settings may need adjustments.\n",
    "\n",
    "ðŸš€ <span style=\"font-size:18px; color:#e63946;\"><b>Let's get started!</b></span> ðŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-size:18px; color:#007acc;\"><b> Table of Contents</b></span>\n",
    "1. [Import Modules](#Import-Modules) \n",
    "2. [Dataset Class](#Dataset-Class)  \n",
    "3. [Define Network](#Define-Network) \n",
    "4. [Dataset Instantiation and Preprocessing](#Dataset-Instantiation-and-Preprocessing) \n",
    "5. [Custom Batch Sampler](#Custom-Batch-Sampler) \n",
    "6. [Training and Validation Functions](#Training-and-Validation-Functions) \n",
    "7. [Evaluation Metrics](#Evaluation-Metrics) \n",
    "8. [Training and Evaluation of Neural Network](#Training-and-Evaluation-of-Neural-Network)\n",
    "9. [SHAP Analysis](#SHAP-Analysis)\n",
    "10. [Save Model](#Save-Model)\n",
    "11. [Create FAST API](#Create-FAST-API)\n",
    "12. [Inference from API](#Inference-from-API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b> 1. Import Modules <a id=\"Import-Modules\"></a> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import fsspec\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "import datetime\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, TensorDataset\n",
    "from torchmetrics import Precision, Recall, F1Score\n",
    "from torch.utils.data import Sampler\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import requests\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from uvicorn import run\n",
    "import threading  # Ensure threading is imported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b> 2. Dataset Class <a id=\"Dataset-Class\"></a> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Define dataset class #####\n",
    "################################\n",
    "# Define a custom dataset class inheriting from PyTorch's Dataset class\n",
    "class FreddieMacDataSet(Dataset):\n",
    "    # Constructor method, used for initializing the dataset\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Read Full 2004 Data\n",
    "        feature_data = pd.read_csv(\"feature2004_7_all.txt\", sep='\\t', skiprows=1)\n",
    "        label_data = pd.read_csv(\"label2004_7_all.txt\", sep='\\t', skiprows=1)\n",
    "        print(feature_data.shape)\n",
    "        print(label_data.shape)\n",
    "        # Convert feature data to a numpy array and store it as X\n",
    "        self.X = np.array(feature_data)\n",
    "        # Convert label data to a numpy array and store it as Y\n",
    "        self.Y = np.array(label_data)\n",
    "        # Store the number of samples in the dataset\n",
    "        self.n_samples = self.X.shape[0]\n",
    "    \n",
    "    # Method to retrieve a sample from the dataset given its index\n",
    "    def __getitem__(self, index):\n",
    "        # Return the feature and label of the sample at the given index\n",
    "        return self.X[index], self.Y[index]\n",
    "\n",
    "    # Method to return the total number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        # Return the number of samples\n",
    "        return self.n_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b> 3. Define Network <a id=\"Define-Network\"></a> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Define network #####\n",
    "##########################\n",
    "class NNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, initialization='he'):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Define the first hidden layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.initialize_weights(self.fc1, initialization)\n",
    "        \n",
    "        # Define subsequent hidden layers\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layer = nn.Linear(hidden_sizes[i], hidden_sizes[i+1])\n",
    "            self.hidden_layers.append(layer)\n",
    "            self.initialize_weights(layer, initialization)\n",
    "        \n",
    "        # Define the output layer\n",
    "        self.fc_out = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        self.initialize_weights(self.fc_out, initialization)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = self.fc1(x)  # Linear activation applied to the output of self.fc1\n",
    "        \n",
    "        # Pass through each hidden layer with decreasing neuron count\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        \n",
    "        # Output layer \n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] \n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "    def initialize_weights(self, layer, initialization):\n",
    "        if initialization == 'uniform':\n",
    "            scale = kwargs.get('scale', 0.1)\n",
    "            nn.init.uniform_(layer.weight, -scale, scale)\n",
    "            nn.init.constant_(layer.bias, 0)\n",
    "        elif initialization == 'normal':\n",
    "            mean = kwargs.get('mean', 0)\n",
    "            std = kwargs.get('std', 0.01)\n",
    "            nn.init.normal_(layer.weight, mean, std)\n",
    "            nn.init.constant_(layer.bias, 0)\n",
    "        elif initialization == 'xavier':\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.constant_(layer.bias, 0)\n",
    "        elif initialization == 'he':\n",
    "            nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "            nn.init.constant_(layer.bias, 0)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid initialization type. Choose from 'uniform', 'normal', 'xavier', or 'he'.\")\n",
    "    \n",
    "    def print_output_layer_weights(self, epoch):\n",
    "        #if epoch % 10 == 0:\n",
    "            last_hidden_layer_weights = self.fc_out.weight.data\n",
    "            print(f\"Epoch {epoch}: fc_out\")\n",
    "            print(last_hidden_layer_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b> 4. Dataset Instantiation and Preprocessing <a id=\"Dataset-Instantiation-and-Preprocessing\"></a> ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Instantiating dataset class #####\n",
    "#######################################\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Instantiate your custom dataset\n",
    "dataset = FreddieMacDataSet()\n",
    "\n",
    "# Split the dataset into features (X) and labels (Y)\n",
    "X, Y = dataset.X, dataset.Y\n",
    "\n",
    "count_of_ones = np.sum(Y == 1)\n",
    "print(count_of_ones)\n",
    "\n",
    "# Perform initial split of the data into training and temporary sets, stratifying by Y\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.30, random_state=42, stratify=Y)\n",
    "\n",
    "# Further split the temporary set into validation and test sets, stratifying by Y_temp\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42, stratify=Y_temp)\n",
    "\n",
    "# Instantiate MinMaxScaler to scale the features\n",
    "sc = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_normalized = sc.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation and test data using the same scaler\n",
    "X_val_normalized = sc.transform(X_val)\n",
    "X_test_normalized = sc.transform(X_test)\n",
    "\n",
    "# Converting to PyTorch tensors\n",
    "train_input_tensor = torch.from_numpy(X_train_normalized).float()\n",
    "train_output_tensor = torch.from_numpy(Y_train).float()\n",
    "valid_input_tensor = torch.from_numpy(X_val_normalized).float()\n",
    "valid_output_tensor = torch.from_numpy(Y_val).float()\n",
    "test_input_tensor = torch.from_numpy(X_test_normalized).float()\n",
    "test_output_tensor = torch.from_numpy(Y_test).float()\n",
    "\n",
    "# Pytorch train, validation, and test sets\n",
    "train = TensorDataset(train_input_tensor, train_output_tensor)\n",
    "valid = TensorDataset(valid_input_tensor, valid_output_tensor)\n",
    "test = TensorDataset(test_input_tensor, test_output_tensor)\n",
    "\n",
    "# Class weights calculation for handling class imbalance\n",
    "train_num_positives = torch.sum(train_output_tensor == 1)\n",
    "train_num_negatives = torch.sum(train_output_tensor == 0)\n",
    "print (\"train_num_positives\", train_num_positives )\n",
    "print(\"train_num_negatives\", train_num_negatives)\n",
    "\n",
    "valid_num_positives = torch.sum(valid_output_tensor == 1)\n",
    "valid_num_negatives = torch.sum(valid_output_tensor == 0)\n",
    "print (\"valid_num_positives\", valid_num_positives )\n",
    "print(\"valid_num_negatives\", valid_num_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b> 5. Custom Batch Sampler <a id=\"Custom-Batch-Sampler\"></a> ##  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Custom Batch Sampler #############################\n",
    "#######################################################################\n",
    "class ProportionalBatchSampler(Sampler):\n",
    "    def __init__(self, data_source, batch_size, state_start_idx=19, state_end_idx=70):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.state_start_idx = state_start_idx\n",
    "        self.state_end_idx = state_end_idx\n",
    "        self.num_samples = len(data_source)\n",
    "        self._prepare_indices()\n",
    "\n",
    "    def _prepare_indices(self):\n",
    "        # For each data point, identify its state\n",
    "        # Assuming data_source.tensors[0] is the input tensor\n",
    "        input_tensor = self.data_source.tensors[0]\n",
    "        states_tensor = input_tensor[:, self.state_start_idx:self.state_end_idx]\n",
    "        state_indices = torch.argmax(states_tensor, dim=1).numpy()\n",
    "\n",
    "        # Collect indices per state\n",
    "        self.state_to_indices = {}\n",
    "        for idx, state in enumerate(state_indices):\n",
    "            if state not in self.state_to_indices:\n",
    "                self.state_to_indices[state] = []\n",
    "            self.state_to_indices[state].append(idx)\n",
    "\n",
    "        # Shuffle indices within each state\n",
    "        for state in self.state_to_indices:\n",
    "            np.random.shuffle(self.state_to_indices[state])\n",
    "\n",
    "        # Calculate the proportion of each state\n",
    "        self.state_proportions = {}\n",
    "        for state in self.state_to_indices:\n",
    "            self.state_proportions[state] = len(self.state_to_indices[state]) / self.num_samples\n",
    "\n",
    "        # Initialize pointers for each state\n",
    "        self.state_pointers = {state: 0 for state in self.state_to_indices}\n",
    "\n",
    "        # Calculate the number of batches\n",
    "        self.num_batches = math.ceil(self.num_samples / self.batch_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.num_batches):\n",
    "            batch_indices = []\n",
    "            for state, proportion in self.state_proportions.items():\n",
    "                num_samples_state = int(round(proportion * self.batch_size))\n",
    "                start_idx = self.state_pointers[state]\n",
    "                end_idx = start_idx + num_samples_state\n",
    "                state_indices = self.state_to_indices[state]\n",
    "\n",
    "                # If not enough samples left in this state, take as many as possible\n",
    "                if end_idx > len(state_indices):\n",
    "                    end_idx = len(state_indices)\n",
    "\n",
    "                batch_indices.extend(state_indices[start_idx:end_idx])\n",
    "                self.state_pointers[state] = end_idx\n",
    "\n",
    "            # If we don't have enough samples in the batch (due to rounding), fill randomly\n",
    "            if len(batch_indices) < self.batch_size:\n",
    "                remaining = self.batch_size - len(batch_indices)\n",
    "                all_indices = []\n",
    "                for state in self.state_to_indices:\n",
    "                    all_indices.extend(self.state_to_indices[state])\n",
    "                np.random.shuffle(all_indices)\n",
    "                batch_indices.extend(all_indices[:remaining])\n",
    "\n",
    "            np.random.shuffle(batch_indices)\n",
    "            yield batch_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b> 6. Training and Validation Functions <a id=\"Training-and-Validation-Functions\"></a> ##  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Define Loss Function and Train Model #####\n",
    "#################################################\n",
    "def train_model(model, device, train_loader, optimizer, pos_weight):\n",
    "    model = model.double() \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    for batch_index, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device).double(), target.to(device).double()  # Convert data and target to double\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader.dataset)  # Average training loss per sample\n",
    "    #     #print(\"Average training loss : {:.4f}\".format(train_loss))\n",
    "    return train_loss\n",
    "\n",
    "####### Compute validation loss #############\n",
    "#############################################\n",
    "def valid_model(model, device, valid_loader, pos_weight):\n",
    "    model.eval()\n",
    "    model = model.to(device).double()\n",
    "    valid_loss = 0.0\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    criterion = criterion.to(device).double()\n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_loader:\n",
    "            data, target = data.to(device).double(), target.to(device).double()\n",
    "            output = model(data)\n",
    "            output = output.to(device).double()\n",
    "            loss = criterion(output, target)\n",
    "            loss = loss.to(device).double()\n",
    "            valid_loss += loss.item() \n",
    "        valid_loss /= len(valid_loader.dataset)\n",
    "        #print(\"Average valid loss: {:.4f}\".format(valid_loss))\n",
    "        return valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b> 7. Evaluation Metrics <a id=\"Evaluation-Metrics\"></a> ##   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Evaluation Metrics ##############\n",
    "#############################################\n",
    "def compute_accuracy(model, data_loader, device):\n",
    "    model = model.to(device).float()\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    CM = torch.zeros(2, 2, dtype=torch.int32)  # Initialize confusion matrix\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = torch.tensor([], dtype=torch.float32, device=device)\n",
    "    all_targets = torch.tensor([], dtype=torch.float32, device=device)\n",
    "\n",
    "    # Compute precision, recall, and F1 score\n",
    "    precision = Precision(average='macro', num_classes=1, task='binary').to(device)\n",
    "    recall = Recall(average='macro', num_classes=1, task='binary').to(device)\n",
    "    f1_score = F1Score(average='macro', num_classes=1, task='binary').to(device)\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients during inference\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device).float(), target.to(device).float()\n",
    "            output = model(data)\n",
    "            predicted = (torch.sigmoid(output) >= 0.5).float()\n",
    "\n",
    "            correct += (predicted == target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "            all_predictions = torch.cat((all_predictions, predicted), dim=0)\n",
    "            all_targets = torch.cat((all_targets, target), dim=0)\n",
    "\n",
    "            CM += torch.tensor(confusion_matrix(target.cpu(), predicted.cpu(), labels=[0, 1]))\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / total * 100  \n",
    "\n",
    "    # Update metrics with predictions and targets\n",
    "    precision.update(all_predictions, all_targets)\n",
    "    recall.update(all_predictions, all_targets)\n",
    "    f1_score.update(all_predictions, all_targets)\n",
    "\n",
    "    # Get the computed values\n",
    "    precision_value = precision.compute().item()\n",
    "    recall_value = recall.compute().item()\n",
    "    f1_score_value = f1_score.compute().item()\n",
    "\n",
    "    true_negative = CM[0][0].item()\n",
    "    true_positive = CM[1][1].item()\n",
    "    false_positive = CM[0][1].item()\n",
    "    false_negative = CM[1][0].item()\n",
    "\n",
    "    return accuracy, precision_value, recall_value, f1_score_value, false_positive, false_negative, true_positive, true_negative, CM\n",
    "\n",
    "\n",
    "########## ROC Function ############\n",
    "####################################\n",
    "\n",
    "def compute_roc(model, data_loader, device):\n",
    "    model = model.to(device).float()\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    all_predictions = torch.tensor([], dtype=torch.float32, device=device)\n",
    "    all_targets = torch.tensor([], dtype=torch.float32, device=device)\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients during inference\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device).float(), target.to(device).float()\n",
    "            output = model(data)\n",
    "            predicted = torch.sigmoid(output)\n",
    "\n",
    "            all_predictions = torch.cat((all_predictions, predicted), dim=0)\n",
    "            all_targets = torch.cat((all_targets, target), dim=0)\n",
    "\n",
    "    # Convert predictions and targets to CPU and numpy arrays\n",
    "    fpr, tpr, thresholds = roc_curve(all_targets.cpu().numpy(), all_predictions.cpu().numpy())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Print number of thresholds\n",
    "    print(f'Number of thresholds: {len(thresholds)}')\n",
    "\n",
    "    # Print thresholds to inspect\n",
    "    print(np.round(thresholds, decimals=4))\n",
    "\n",
    "    return fpr, tpr, roc_auc, thresholds\n",
    "\n",
    "###### PRC Function ##############\n",
    "##################################\n",
    "\n",
    "def compute_prc(model, data_loader, device):\n",
    "    model = model.to(device).float()\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    all_predictions = torch.tensor([], dtype=torch.float32, device=device)\n",
    "    all_targets = torch.tensor([], dtype=torch.float32, device=device)\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients during inference\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device).float(), target.to(device).float()\n",
    "            output = model(data)\n",
    "            predicted = torch.sigmoid(output)\n",
    "\n",
    "            all_predictions = torch.cat((all_predictions, predicted), dim=0)\n",
    "            all_targets = torch.cat((all_targets, target), dim=0)\n",
    "\n",
    "    # Convert predictions and targets to CPU and numpy arrays\n",
    "    precision, recall, thresholds = precision_recall_curve(all_targets.cpu().numpy(), all_predictions.cpu().numpy())\n",
    "    prc_auc = auc(recall, precision)\n",
    "    \n",
    "    return precision, recall, prc_auc, thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b> 8. Training and Evaluation of Neural Network <a id=\"Training-and-Evaluation-of-Neural-Network\"></a> ##    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Main program ##############\n",
    "##################################\n",
    "# Define the device\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 50\n",
    "input_size = 23\n",
    "weight_decay = 0.0\n",
    "hidden_sizes = [2560, 1280, 640, 320, 160, 80, 40, 20, 10, 5]\n",
    "output_size = 1\n",
    "learning_rate = 0.0002  # Fixed learning rate\n",
    "pos_weights = [2.0]  # Iterate over positive weights\n",
    "batch_percentage = 0.02\n",
    "\n",
    "# Calculate batch size\n",
    "total_train_samples = len(train)\n",
    "batch_size = int(total_train_samples * batch_percentage)\n",
    "batch_size = max(1, batch_size)  # Ensure at least 1\n",
    "\n",
    "# Initialize dictionaries to store values for plotting later\n",
    "all_training_losses = {}\n",
    "all_valid_losses = {}\n",
    "all_train_accuracies = {}\n",
    "all_valid_accuracies = {}\n",
    "all_train_false_positives = {}\n",
    "all_train_false_negatives = {}\n",
    "all_valid_false_positives = {}\n",
    "all_valid_false_negatives = {}\n",
    "all_train_fpr = {}\n",
    "all_train_tpr = {}\n",
    "all_train_roc_auc = {}\n",
    "all_valid_fpr = {}\n",
    "all_valid_tpr = {}\n",
    "all_valid_roc_auc = {}\n",
    "all_train_precision = {}\n",
    "all_train_recall = {}\n",
    "all_valid_precision = {}\n",
    "all_valid_recall = {}\n",
    "all_train_prc_auc = {}\n",
    "all_valid_prc_auc = {}\n",
    "all_train_thresholds = {}\n",
    "all_valid_thresholds = {}\n",
    "\n",
    "# Converting to DataLoaders\n",
    "train_sampler = ProportionalBatchSampler(train, batch_size=batch_size)\n",
    "train_loader = DataLoader(train, batch_sampler=train_sampler)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate through positive weights\n",
    "for pos_weight_mul in pos_weights:\n",
    "    # Calculate the total number of samples\n",
    "    pos_count = torch.sum(train_output_tensor == 1).item()\n",
    "    neg_count = torch.sum(train_output_tensor == 0).item()\n",
    "\n",
    "    # Check for zero positive samples\n",
    "    if pos_count == 0:\n",
    "        raise ValueError(\"No positive samples in the dataset, cannot compute pos_weight.\")\n",
    "\n",
    "    # Calculate the positive weight\n",
    "    pos_weight = (neg_count / pos_count) * pos_weight_mul\n",
    "    pos_weight = torch.tensor([pos_weight], device=device)\n",
    "\n",
    "    # Reset lists for the current positive weight\n",
    "    Epoch_ind = []\n",
    "    training_losses = []\n",
    "    valid_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_accuracies = []\n",
    "    train_false_positives = []\n",
    "    train_false_negatives = []\n",
    "    valid_false_positives = []\n",
    "    valid_false_negatives = []\n",
    "\n",
    "    # Initialize model\n",
    "    model = NNModel(input_size, hidden_sizes, output_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 25, 50], gamma=0.1)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train_model(model, device, train_loader, optimizer, pos_weight)\n",
    "        training_losses.append(train_loss)\n",
    "\n",
    "        train_accuracy, _, _, _, train_false_positive, train_false_negative, train_true_positive, train_true_negative, _ = compute_accuracy(model, train_loader, device)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        train_false_negatives.append(train_false_negative)\n",
    "        train_false_positives.append(train_false_positive)\n",
    "\n",
    "        valid_loss = valid_model(model, device, valid_loader, pos_weight)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        valid_accuracy, _, _, _, valid_false_positive, valid_false_negative, valid_true_positive, valid_true_negative, _ = compute_accuracy(model, valid_loader, device)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "        valid_false_negatives.append(valid_false_negative)\n",
    "        valid_false_positives.append(valid_false_positive)\n",
    "\n",
    "        Epoch_ind.append(epoch)\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        print ('epoch '+ str(epoch))\n",
    "\n",
    "    # Compute ROC curve\n",
    "    train_fpr, train_tpr, train_roc_auc, train_thresholds = compute_roc(model, train_loader, device)\n",
    "    valid_fpr, valid_tpr, valid_roc_auc, valid_thresholds = compute_roc(model, valid_loader, device)\n",
    "    \n",
    "    # Compute PRC\n",
    "    train_precision, train_recall, train_prc_auc, train_thresholds = compute_prc(model, train_loader, device)\n",
    "    valid_precision, valid_recall, valid_prc_auc, valid_thresholds = compute_prc(model, valid_loader, device)\n",
    "    \n",
    "     # Store results for the current positive weight\n",
    "    all_training_losses[pos_weight_mul] = training_losses\n",
    "    all_valid_losses[pos_weight_mul] = valid_losses\n",
    "    all_train_accuracies[pos_weight_mul] = train_accuracies\n",
    "    all_valid_accuracies[pos_weight_mul] = valid_accuracies\n",
    "    all_train_false_positives[pos_weight_mul] = train_false_positives\n",
    "    all_train_false_negatives[pos_weight_mul] = train_false_negatives\n",
    "    all_valid_false_positives[pos_weight_mul] = valid_false_positives\n",
    "    all_valid_false_negatives[pos_weight_mul] = valid_false_negatives\n",
    "    \n",
    "    all_train_fpr[pos_weight_mul] = train_fpr\n",
    "    all_train_tpr[pos_weight_mul] = train_tpr\n",
    "    all_train_roc_auc[pos_weight_mul] = train_roc_auc\n",
    "    all_valid_fpr[pos_weight_mul] = valid_fpr\n",
    "    all_valid_tpr[pos_weight_mul] = valid_tpr\n",
    "    all_valid_roc_auc[pos_weight_mul] = valid_roc_auc\n",
    "\n",
    "    all_train_precision[pos_weight_mul] = train_precision\n",
    "    all_train_recall[pos_weight_mul] = train_recall\n",
    "    all_valid_precision[pos_weight_mul] = valid_precision\n",
    "    all_valid_recall[pos_weight_mul] = valid_recall\n",
    "    \n",
    "    all_train_prc_auc[pos_weight_mul] = train_prc_auc\n",
    "    all_valid_prc_auc[pos_weight_mul] = valid_prc_auc\n",
    "\n",
    "    all_train_thresholds[pos_weight_mul] = train_thresholds\n",
    "    all_valid_thresholds[pos_weight_mul] = valid_thresholds\n",
    "\n",
    "# Plotting results for all positive weights\n",
    "plt.figure(figsize=(15, 25))\n",
    "\n",
    "# Plot Train False Positives vs Epochs\n",
    "plt.subplot(5, 2, 1)\n",
    "for pos_weight_mul in pos_weights:\n",
    "    plt.plot(Epoch_ind, all_train_false_positives[pos_weight_mul], label=f\"Pos Weight {pos_weight_mul}\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Train False Positives')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Train False Negatives vs Epochs\n",
    "plt.subplot(5, 2, 2)\n",
    "for pos_weight_mul in pos_weights:\n",
    "    plt.plot(Epoch_ind, all_train_false_negatives[pos_weight_mul], label=f\"Pos Weight {pos_weight_mul}\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Train False Negatives')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Valid False Positives vs Epochs\n",
    "plt.subplot(5, 2, 3)\n",
    "for pos_weight_mul in pos_weights:\n",
    "    plt.plot(Epoch_ind, all_valid_false_positives[pos_weight_mul], label=f\"Pos Weight {pos_weight_mul}\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Valid False Positives')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Valid False Negatives vs Epochs\n",
    "plt.subplot(5, 2, 4)\n",
    "for pos_weight_mul in pos_weights:\n",
    "    plt.plot(Epoch_ind, all_valid_false_negatives[pos_weight_mul], label=f\"Pos Weight {pos_weight_mul}\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Valid False Negatives')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Training and Validation Loss vs Epochs\n",
    "plt.subplot(5, 2, 5)\n",
    "for pos_weight_mul in pos_weights:\n",
    "    plt.plot(Epoch_ind, all_training_losses[pos_weight_mul], label=f\"Train Loss (Pos Weight {pos_weight_mul})\")\n",
    "    plt.plot(Epoch_ind, all_valid_losses[pos_weight_mul], label=f\"Valid Loss (Pos Weight {pos_weight_mul})\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot ROC curve for the last epoch\n",
    "plt.subplot(5, 2, 6)\n",
    "for pos_weight_mul in pos_weights:\n",
    "    plt.plot(all_train_fpr[pos_weight_mul], all_train_tpr[pos_weight_mul], label=f'Train ROC Curve (AUC = {all_train_roc_auc[pos_weight_mul]:.2f}, Pos Weight {pos_weight_mul})')\n",
    "    plt.plot(all_valid_fpr[pos_weight_mul], all_valid_tpr[pos_weight_mul], label=f'Validation ROC Curve (AUC = {all_valid_roc_auc[pos_weight_mul]:.2f}, Pos Weight {pos_weight_mul})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve (Last Epoch)')\n",
    "plt.legend()\n",
    "\n",
    "# Plot PRC Curve\n",
    "plt.subplot(5, 2, 7)\n",
    "for pos_weight_mul in pos_weights:\n",
    "    plt.plot(all_train_recall[pos_weight_mul], all_train_precision[pos_weight_mul], label=f'Train PR Curve (AUC = {all_train_prc_auc[pos_weight_mul]:.2f}, Pos Weight {pos_weight_mul})')\n",
    "    plt.plot(all_valid_recall[pos_weight_mul], all_valid_precision[pos_weight_mul], label=f'Validation PR Curve (AUC = {all_valid_prc_auc[pos_weight_mul]:.2f}, Pos Weight {pos_weight_mul})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('PR Curve (Last Epoch)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b> 9. SHAP Analysis <a id=\"SHAP-Analysis\"></a> ##    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SHAP Analysis ####\n",
    "######################\n",
    "import shap\n",
    "def shap_analysis(model, input_tensor):\n",
    "    # Ensure model is in evaluation mode and on CPU\n",
    "    model.eval()\n",
    "    model = model.cpu()\n",
    "\n",
    "    # Create SHAP explainer\n",
    "    explainer = shap.DeepExplainer(model, input_tensor)\n",
    "\n",
    "    # Compute SHAP values\n",
    "    shap_values = explainer.shap_values(input_tensor, check_additivity = False)\n",
    "    \n",
    "    return shap_values\n",
    "\n",
    "# Perform SHAP analysis\n",
    "subset_tensor = valid_input_tensor[:10000]\n",
    "model.eval()\n",
    "model = model.cpu()\n",
    "model_predictions = model(subset_tensor).detach().numpy()[:, 0]\n",
    "#print(model_predictions)\n",
    "\n",
    "shap_values = shap_analysis(model,subset_tensor)\n",
    "shap_values = np.squeeze(shap_values)  # This will remove all dimensions of size 1\n",
    "\n",
    "shap_values = np.array(shap_values)\n",
    "# Sum over the feature dimension (axis=1)\n",
    "instance_sum = np.sum(shap_values, axis=1)\n",
    "#print(\"Sum for each instance (shape (2,)):\")\n",
    "#print(instance_sum)\n",
    "\n",
    "#shap_exp = shap.Explanation(shap_values, feature_names=[\"Feature_\" + str(i) for i in range(train_input_tensor.size(1))])\n",
    "shap_exp = shap.Explanation(shap_values, feature_names=['Credit Score', 'First Time Homebuyer Flag', 'MSA', 'No of Units', 'Occupancy Status', \n",
    "                                                        'CLTV', 'DTI Ratio', 'Ori UPB', 'Ori Interest Rate', 'Channel', 'Amortization Type', \n",
    "                                                        'Property State', 'Property Type', 'Loan Purpose', 'Ori Loan Term', 'Number of Borrowers',\n",
    "                                                        'Servicer Name', 'I/O Indicator', 'MI Cancellation Indicator', 'Current actual UPB',\n",
    "                                                        'loan age', 'Remaining months to legal maturity', 'Current interest rate'])\n",
    "\n",
    "\n",
    "# Generate the summary plot\n",
    "shap.summary_plot(shap_exp.values, features=subset_tensor.cpu().numpy(), feature_names=shap_exp.feature_names)\n",
    "# Now, you can plot using the bar plot\n",
    "shap.plots.bar(shap_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b> 10. Save Model <a id=\"Save-Model\"></a> ##    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b> 11. Create FAST API <a id=\"Create-FAST-API\"></a> ##    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming NNModel is already defined earlier in the notebook\n",
    "# If not, make sure to define it before using it here\n",
    "\n",
    "# Initialize the FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define a Pydantic model for input data\n",
    "class InputData(BaseModel):\n",
    "    features: list\n",
    "\n",
    "# Initialize your model with the same parameters as during training\n",
    "output_size = 23\n",
    "hidden_sizes = [2560, 1280, 640, 320, 160, 80, 40, 20, 10, 5]\n",
    "output_size = 1\n",
    "\n",
    "# Create the model instance\n",
    "model = NNModel(input_size, hidden_sizes, output_size, initialization)\n",
    "model.load_state_dict(torch.load('model_weights.pth'))  # Load pre-trained weights\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(data: InputData):\n",
    "    # Convert input data to tensor and ensure it has the correct shape\n",
    "    input_tensor = torch.tensor(data.features, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor)\n",
    "    return {\"prediction\": prediction.tolist()}\n",
    "\n",
    "# Function to run the app in a separate thread\n",
    "def run_app():\n",
    "    run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "\n",
    "# Start the FastAPI server in a separate thread\n",
    "thread = threading.Thread(target=run_app)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:18px; color:#007acc;\"><b> 12. Inference from API <a id=\"Inference-from-API\"></a> ##    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first example (row) from test_input_tensor and convert it to a list\n",
    "features = test_input_tensor[:1].tolist()[0]  # Extracts the first example and converts it to a list\n",
    "\n",
    "# Extract the true label corresponding to the first example\n",
    "true_label = test_output_tensor[0].item()  # Converts the tensor value to a Python scalar\n",
    "\n",
    "# Define the endpoint\n",
    "url = \"http://localhost:8000/predict/\"\n",
    "\n",
    "data = {\"features\": features}  # Use the extracted features\n",
    "\n",
    "try:\n",
    "    # Make the POST request\n",
    "    response = requests.post(url, json=data)\n",
    "    \n",
    "    # Print status code\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    \n",
    "    # Print response text for debugging\n",
    "    print(f\"Response Text: {response.text}\")\n",
    "\n",
    "    # Assuming the response is a JSON array of logits\n",
    "    logits = response.json()\n",
    "\n",
    "    # Apply the sigmoid function and threshold to each logit\n",
    "    prediction = sigmoid(logits)\n",
    "    prediction = [1 if prediction >= 0.5 else 0 for logit in logits]\n",
    "\n",
    "    # Print the resulting predictions (0 or 1)\n",
    "    print(\"Predicted Labels:\", predictions)\n",
    "\n",
    "    # Print the true label\n",
    "    print(\"True Label:\", true_label)\n",
    "    \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request failed: {e}\")\n",
    "except ValueError as e:\n",
    "    print(f\"JSON decode error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
